                        The Public Domain Ratfor test suite


User interface
==============


Running all tests
-----------------

  make check

  You can use `-jN' for faster completion (roughly, this runs up to N
  tests simultaneously, in parallel).

  Tests are normally runned in paralle


Interpretation
--------------

  Successes:
    PASS  - success
    XFAIL - expected failure

  Failures:
    FAIL  - failure
    XPASS - unexpected success

  Other:
    SKIP  - skipped tests (third party tools not available)


Getting details from failures
-----------------------------

  Each test is a script.  By default, verbose output of a test foo.test
  is retained in the log file foo.log.  A summary log is created in the
  file test-suite.log.  In a non-VPATH build you can also run test scripts
  directly (with the current directory being necessarly the the `tests/'
  subdirectory), they will be verbose and no log file will be written.
  You can limit the set of files using the TESTS variable, and enable
  detailed test output at the end of the test run with the VERBOSE
  variable:

    make VERBOSE=yes TESTS='first.test second.test ...' check
  
  The above command works only if you are in the `tests/' subdirectory;
  if you want to do something similar while in the top directory, you'll
  have to resort to e.g.

    env VERBOSE=yes TESTS='first.test second.test ...' make -e check

  But remember that `make -e' can be evil, since it could pull to much
  things in the makefile's namespace.  You've been warned.


Preserving temporary files
--------------------------
  
  By default, every test case `foo.test' create a temporary directory
  named `foo.dir', and remove it on sucessfull termination.
  Use `keep_testdirs=yes' to keep test directories for successful tests
  also.


Reporting failures
------------------

  Send verbose output, i.e., the contents of test-suite.log, of failing
  tests to <stefano.lattarini@gmail.com>, along with the usual version
  numbers (which ratfor, which C compiler, which operating system, which
  make utility and version, which shell, etc.)



Writing test cases
==================


Do
--
  
  Make sure the new test is executable.  Add it to TESTS in Makefile.am,
  add it to XFAIL_TESTS in addition if needed.  If it depends on a data
  file (say `foo.data'), add the proper dependency in Makefile.am, e.g.
  `foo.log: foo.data', and add `foo.data' to EXTRA_DIST.
  
  Keep the shell code as portable as possible.  This can be trickier that
  you think; for more information, you can refer to the Autoconf manual,
  especially the section `Portable shell programming' -- which should be
  reachable at the following URL:
  http://www.gnu.org/software/autoconf/manual/html_node/Portable-Shell.html

  Include `./defs.sh', aborting the test if the inclusion fails.  You'd
  better read and understand that file well before trying to write a
  test case.

  Use `Exit' rather than `exit' to terminate a test. Do *not* use Exit
  in a subshell.

  Every test must use a call to `Exit' (either to indicate SUCCESS, FAIL
  or SKIP) to do normal termination.  If a test exit for another reason
  (e.g. a call to plain `exit', a failed command when the `set -e' switch
  is active, a termination signal, a syntax error in shell code, ...) the
  test will end in an "hard error", and will be considered FAIL'd.

  Use `set -e' to catch failures you might not have thought of.
  
  If you plan to fix a bug, write the test case first.  This way you'll
  make sure the test catches the bug, and that it succeeds once you have
  fixed the bug.

  Explain what the test does.

  Cite the PR number (if any), and the original reporter (if any), so
  we can find or ask for information if needed.

  Use $GREP, $FGREP, $EFREP, $SED, $AWK, $SHELL instead of the
  corresponding commands.  Also prefer $DIFF_U on plain `diff'.
  This list of tools might be incomplete: look at defs.in for a
  complete list.

  Use `cat' or `grep' to display (part of) files that may be
  interesting for debugging, so that when a user send a verbose
  output we don't have to ask him for more details.  Display stderr
  output on the stderr file descriptor.  If some redirected command
  is likely to fail, and `set -e' is in effect, display its output
  even in the failure case, before exiting.

  It's more important to make sure that a feature works, than
  make sure that ratfor's output looks correct.  It might look
  correct and still fail to work.  In other words, prefer compiling and
  running the fotran code generated by ratfor over grepping it (or do
  both).

  If you plan to test a ratfor feature (let's call it "foo") by both
  grepping its output and testing the generated fortran code, divide the
  test into two scripts, say `scripts/foo.test' and `scripts/foo_f77.test',
  and place any shared bata between them in `data/foo.data' (if you need
  more data files, use `data/foo.data1', `data/foo.data2' etc. instead).
  This is done so that, even when a Fortran 77 compiler is not available,
  the tests about the feature "foo" will not be completely skipped (only
  "half-skipped").  Finally, in `scripts/foo_f77.test', place a call to
  `require_fortran_compiler' just after the inclusion of `./defs.sh', so
  that the test will be skipped if no Fortran 77 compiler is available.
  

Do not
------

  Keep the different test cases indipendent one from another.  And above
  all, do not rely on a particular order in tests' execution; even if they
  are usually run in alphabetic order, this might likely no more hold when
  running them in parallel.

  Do not try to be too smart when writing a testcase: better keep it
  simple and robust. But do not dumb down a test to the point of making
  it incomplete or sloppy.

  Do not test a ratfor error with `$RATFOR && Exit 1', since the ratfor
  program might fail for a reason different from the one you thought of.
  Better use something like:
     run_RATFOR -e 1 || Exit 1
     grep 'expected diagnostic' stderr || Exit 1
  (Note that this doesn't prevent the test from failing for another
  reason, but at least it makes sure the original error is still there.)

---

$Id$
