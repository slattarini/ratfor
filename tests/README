                        The Public Domain Ratfor test suite


User interface
==============


Running all tests
-----------------

  make check

  You can use `-jN' for faster completion (roughly, this runs up to N
  tests simultaneously, in parallel).

  Tests are normally runned in paralle


Interpretation
--------------

  Successes:
    PASS  - success
    XFAIL - expected failure

  Failures:
    FAIL  - failure
    XPASS - unexpected success

  Other:
    SKIP  - skipped tests (third party tools not available)


Getting details from failures
-----------------------------

  Each test is a script.  In a non-VPATH build you can run them directly,
  they will be verbose.  By default, verbose output of a test foo.test is
  retained in the log file foo.log.  A summary log is created in the file
  test-suite.log.

  You can limit the set of files using the TESTS variable, and
  enable detailed test output at the end of the test run with the
  VERBOSE variable:

    make VERBOSE=yes TESTS='first.test second.test ...' check
  
  The above command works only if you are in the `tests/' subdirectory;
  if you want to do something similar while in the top directory, you'll
  have to resort to e.g.

    env VERBOSE=yes TESTS='first.test second.test ...' make -e check

  But remember that `make -e' can be evil, since it could pull to much
  things in the makefile's namespace.  You've been warned.


Preserving temporary files
--------------------------
  
  By default, every test case `foo.test' create a temporary directory
  named `foo.dir', and remove it on sucessfull termination.
  Use `keep_testdirs=yes' to keep test directories for successful tests
  also.


Reporting failures
------------------

  Send verbose output, i.e., the contents of test-suite.log, of failing
  tests to <stefano.lattarini@gmail.com>, along with the usual version
  numbers (which ratfor, which C compiler, which operating system, which
  make utility and version, which shell, etc.)



Writing test cases
==================


Do
--
  
  Keep the shell code as portable as possible.  This can be trickier that
  you think; for more information, you can refer to the Autoconf manual,
  especially the section `Portable shell programming' -- which should be
  reachable at the following URL:
  http://www.gnu.org/software/autoconf/manual/html_node/Portable-Shell.html

  Use `Exit' rather than `exit' to terminate a test. Do *not* use Exit
  in a subshell.

  Every test must use a call to `Exit' (either to indicate SUCCESS, FAIL
  or SKIP) to do normal termination.  If a test exit for another reason
  (e.g. a call to plain `exit', a failed command when the `set -e' switch
  is active, a termination signal, a syntax error in shell code, ...) the
  test will end in an "hard error", and will be considered FAIL'd.

  If you plan to fix a bug, write the test case first.  This way you'll
  make sure the test catches the bug, and that it succeeds once you have
  fixed the bug.

  Explain what the test does.

  Cite the PR number (if any), and the original reporter (if any), so
  we can find or ask for information if needed.

  Include ./defs.sh, aborting the test if the inclusion fails.  You'd
  better read and understand that file well before trying to write a
  test case.

  Use `set -e' to catch failures you might not have thought of.

  Use $GREP, $FGREP, $EFREP, $SED, $AWK, $SHELL instead of the
  corresponding commands.  Also prefer $DIFF_U on plain `diff'.
  This list of tools might be incomplete: look at defs.in for a
  complete list.

  Use `cat' or `grep' to display (part of) files that may be
  interesting for debugging, so that when a user send a verbose
  output we don't have to ask him for more details.  Display stderr
  output on the stderr file descriptor.  If some redirected command
  is likely to fail, and `set -e' is in effect, display its output
  even in the failure case, before exiting.

  It's more important to make sure that a feature works, than
  make sure that ratfor's output looks correct.  It might look
  correct and still fail to work.  In other words, prefer compiling and
  running the fotran code generated by ratfor over grepping it (or do
  both).

  Before commit: make sure the test is executable, add the tests to
  TESTS in Makefile.am, add it to XFAIL_TESTS in addition if needed.


Do not
------

  Keep the different test cases indipendent one from another.  And above
  all, do not rely on a particular order in tests' execution; even if they
  are usually run in alphabetic order, this might likely no more hold when
  running them in parallel.

  Do not try to be too smart when writing a testcase: better keep it
  simple and robust. But do not dumb down a test to the point of making
  it incomplete or sloppy.

  Do not test a ratfor error with `$RATFOR && Exit 1', since the ratfor
  program might fail for a reason different from the one you thought of.
  Better use something like:
     run_RATFOR -e 1
     grep 'expected diagnostic' stderr
  (Note that this doesn't prevent the test from failing for another
  reason, but at least it makes sure the original error is still there.)

---

$Id$
