                        The Public Domain Ratfor test suite


User interface
==============


Running all tests
-----------------

  make check

  You can use `-jN' for faster completion (roughly, this runs up to N
  tests simultaneously, in parallel).

  Tests are normally runned in paralle


Interpretation
--------------

  Successes:
    PASS  - success
    XFAIL - expected failure

  Failures:
    FAIL  - failure
    XPASS - unexpected success

  Other:
    SKIP  - skipped tests (third party tools not available)


Fortran 77 Compiler
-------------------
  
  Some tests require a Fortran 77 compiler.  If it is not present, they
  should be automatically skipped.
  
  By default, configure scans your system to found an available compiler,
  but this can be overridden by passing it a proper value for the variable
  `RAT4_TESTSUITE_F77', as in:
    
    ./configure RAT4_TESTSUITE_F77=/usr/local/gnu/bin/g77
  
  A special value of `NONE' for `RAT4_TESTSUITE_F77' will explicitly tell
  configure that you don't have any avaiable Fortran 77 compiler.  All the
  tests requiring such a compiler will be skipped.


Getting details from failures
-----------------------------

  Each test is a shell script, and by default is run by $CONFIG_SHELL
  (which should be the best shell found at configure time).
  By default, verbose output of a test `foo.test' is retained in the log
  file `foo.log'.  A summary log is created in the file `test-suite.log'.
  
  In a non-VPATH build you can also run test scripts directly (with the
  current directory being necessarly the the `tests/' subdirectory), they
  will be verbose and no log file will be written.
  
  You can limit the set of files using the TESTS variable, and enable
  detailed test output at the end of the test run with the VERBOSE
  variable:

    make VERBOSE=yes TESTS='first.test second.test ...' check
  
  The above command works only if you are in the `tests/' subdirectory;
  if you want to do something similar while in the top directory, you'll
  have to resort to e.g.

    env VERBOSE=yes TESTS='first.test second.test ...' make -e check

  But remember that `make -e' can be evil, since it could pull to much
  things in the makefile's namespace.  You've been warned.


Supported shells
----------------

  The test scripts are written with portability in mind, so that they
  should run an any decent bourne-compatible shell.
  
  However, some care must be used with Zsh, since, when not directly
  starting in bourne-compatibility mode, it has some incompatibilities
  in the handling of `$0' which conflict with our usage, and which have
  no easy workaround.  Thus, if you want to run a test script, say
  foo.test, with Zsh, you *can't* simply do `zsh foo.test', but you
  *must* resort to:
    zsh --no-function-argzero foo.test

  Note that this problem does not occur if zsh is executed through a
  symlink with `sh' basename, since in that case it immediatly starts
  in bourne-compatibility mode.  So you should be perfectly safe even
  having zsh as your /bin/sh.


Preserving temporary files
--------------------------
  
  By default, every test case `foo.test' create a temporary directory
  named `foo.dir', and remove it on sucessfull termination.
  Use `keep_testdirs=yes' to keep test directories for successful tests
  also.


Reporting failures
------------------

  Send verbose output, i.e., the contents of test-suite.log, of failing
  tests to <stefano.lattarini@gmail.com>, along with the usual version
  numbers (which ratfor, which C compiler, which operating system, which
  make utility and version, which shell, etc.)



Writing test cases
==================


Do
--
  
  Make sure the new test is executable.  Add it to TESTS in Makefile.am,
  add it to XFAIL_TESTS in addition if needed.  If it depends on a data
  file (say `foo.data'), add the proper dependency in Makefile.am, e.g.
  `foo.log: foo.data', and add `foo.data' to EXTRA_DIST.
  
  Include `./defs.sh', aborting the test (with exit status `99') if the
  inclusion fails.  You'd better read and understand that file well before
  trying to write a test case.

  Use either `testcase_PASS', `testcase_FAIL', `testcase_SKIP' or
  `testcase_HARDERROR' to terminate a test. If for some reason you need
  to return with a custom exit status, use `Exit' rather than `exit'.
  Do *not* use the `testcase_*' or `Exit' subroutines in a subshell.
  *Every* test must use a call to either `testcase_PASS', `testcase_FAIL',
  `testcase_SKIP', `testcase_HARDERROR' or `Exit' to do normal termination.
  If a test exits for another reason (e.g. a call to plain `exit', a failed
  command when the `set -e' switch is active, a terminating signal, a
  syntax error in shell code, ...) the test will end in an "hard error",
  and will be considered FAIL'd.

  Use `set -e' to catch failures you might not have thought of.
  
  Keep the shell code as portable as possible.  This can be trickier that
  you think; for more information, you can refer to the Autoconf manual,
  especially the section `Portable shell programming' -- which should be
  reachable at the following URL:
  http://www.gnu.org/software/autoconf/manual/html_node/Portable-Shell.html
  
  Explain what the test does, at least briefly.

  Use $GREP, $FGREP, $EFREP, $SED, $AWK, $SHELL instead of the
  corresponding commands.  Also prefer $DIFF_U on plain `diff'.
  This list of tools might be incomplete: look at defs.in for a
  complete list.

  When calculating diffs on expected output and obtained output, put
  file containing expected output first, as in `$DIFF_U exp.txt got.txt'.
  This will make the output of diff more intuitive and easier to grasp.

  If you plan to fix a bug, write the test case first.  This way you'll
  make sure the test catches the bug, and that it succeeds once you have
  fixed the bug.  Also, cite the PR number (if any), and the original
  reporter (if any), so we can find or ask for information if needed.

  Use `cat' or `grep' to display (part of) files that may be
  interesting for debugging, so that when a user send a verbose
  output we don't have to ask him for more details.  Display stderr
  output on the stderr file descriptor.  If some redirected command
  is likely to fail, and `set -e' is in effect, display its output
  even in the failure case, before exiting.

  It's more important to make sure that a feature works, than
  make sure that ratfor's output looks correct.  It might look
  correct and still fail to work.  In other words, prefer compiling and
  running the fotran code generated by ratfor over grepping it (or do
  both).

  If you plan to test a ratfor feature (let's call it "foo") by both
  grepping its output and testing the generated fortran code, divide the
  test into two scripts, say `scripts/foo.test' and `scripts/foo_f77.test',
  and place any shared bata between them in `data/foo.data' (if you need
  more data files, use `data/foo.data1', `data/foo.data2' etc. instead).
  This is done so that, even when a Fortran 77 compiler is not available,
  the tests about the feature "foo" will not be completely skipped (only
  "half-skipped").  Finally, in `scripts/foo_f77.test', place a call to
  `require_fortran_compiler' just after the inclusion of `./defs.sh', so
  that the test will be skipped if no Fortran 77 compiler is available.
  

Do not
------

  Do not make different test cases dependent one from another.  And above
  all, do not rely on a particular order in tests' execution; even if they
  are usually run in alphabetic order, this might likely no more hold when
  running them in parallel.

  Do not try to be too smart when writing a testcase: better keep it
  simple and robust. But do not dumb down a test to the point of making
  it incomplete or sloppy.

  Do not test a ratfor error with `$RATFOR && Exit 1', since the ratfor
  program might fail for a reason different from the one you thought of.
  Better use something like:
     run_RATFOR -e 1 || Exit 1
     grep 'expected diagnostic' stderr || Exit 1
  (Note that this doesn't prevent the test from failing for another
  reason, but at least it makes sure the original error is still there.)

---

$Id$
