                           The Ratfor Testsuite


Brief Description
=================

This package contains a set of blackbox testcases for the ratfor
preprocessor.  It is an independent package, not tailored to a particular
ratfor implementation.

It can be used to check that ratfor implmentations has "standardized"
behaviour (where the "standardized" is to be interpreted as "passes all
the tests defined in the Ratfor Testsuite").

This package is used in particular as the testsuite of the Public Domain
Ratfor (a ratfor proprocessor written in C and released in the public
domain).



User interface
==============


Running all tests
-----------------
  
  The command:

    make check
  
  should run all the ratfor testcases.  They will be run in series.
  
  You can use the `-j N' make option for faster completion (roughly, this
  runs up to N tests simultaneously, in parallel).  An example:
    
    make -j 5 check


Interpretation
--------------

  Successes:
    PASS  - success
    XFAIL - expected failure

  Failures:
    FAIL  - failure
    XPASS - unexpected success

  Other:
    SKIP  - skipped tests (third party tools not available)


Ratfor preprocessor
-------------------
  
  By default, configure scans your system to found an available ratfor
  preprocessor, but this can be overridden by passing it a proper value
  for the variable `RATFOR', as in:
    
    ./configure RATFOR=/opt/bin/ratfor
  
  Note that, obviously, a ratfor proprocessor is a necessary preprequisite
  to run the ratfor testsuite, and thus configure aborts if it fails to
  find one.

  More importatnly, `RATFOR' is a "configure precious variable", so that
  it can be passed to configure as an environment variable without creating
  subtle problems:
    
    export RATFOR=$HOME/bin/my-ratfot; ./configure
  
  This might not be particularly useful to individual usesr, but is *very*
  useful for a package providing a ratfor implementation and using the
  present "Ratfor Testsuite" package as a subpackage implementing a
  black-box testsuite.
  
  For example, the configure script of such a package can:
   
   1. set the $RATFOR shell variable to the absolute path of the ratfor
      preprocessor it provides;
   2. export this varible in the environment;
   3. recursively call the configure script of the "Ratfor Testsuite"
      subpackage, which will then read the path of the ratfor preprocessor
      from the $RATFOR variable passed in the environment (this recursive
      call is supported almost "out fo the box" if the top-level configure
      script is generated by autoconf).
  
  After that, the "Ratfor Testsuite" supbackage will be configured to use
  the ratfor preprocessor provided by the top-level package.

  The strategy just described is exactly the one employed the the "Public
  Domain Ratfor" package; for clarity, an excerpt from its `configure.ac'
  file is presented (note that the "Public Domain Ratfor" package keeps
  the "Ratfor Testsuite" subpackage in the `blackbox-tests' subdirectory):

    ...
    AC_INIT([Public Domain Ratfor], ...)
    ... 
    # the testsuite is kept in a separate subpackage.
    AC_CONFIG_SUBDIRS([blackbox-tests])
    # Since `RATFOR' is declared in `blackbox-tests/configure.ac' as a
    # precious variable, this will pass the proper value of $RATFOR to
    # the recusrively invoked script `blackbox-tests/configure'.
    # NOTE: when the configure script is running, the current working
    # directory (returned by `pwd') is the build directory.
    RATFOR=`pwd`/src/ratfor; export RATFOR
    AC_MSG_NOTICE([exported RATFOR='$RATFOR'])
    ...


Fortran 77 Compiler
-------------------
  
  Some tests require a Fortran 77 compiler.  If it is not present, they
  should be automatically skipped.
  
  By default, configure scans your system to found an available compiler,
  but this can be overridden by passing it a proper value for the variable
  `F77', as in:
    
    ./configure F77=/usr/local/gnu/bin/g77
  
  A special value of `NONE' for `F77' will explicitly tell configure that
  you don't have any avaiable Fortran 77 compiler.  All the tests requiring
  such a compiler will be skipped.
  

Getting details from failures
-----------------------------

  Each test is a shell script, and by default is run by $CONFIG_SHELL
  (which should be the best shell found at configure time).
  By default, verbose output of a test `foo.test' is retained in the log
  file `foo.log'.  A summary log is created in the file `test-suite.log'.
  
  In a non-VPATH build you can also run test scripts directly (with the
  current directory being necessarly the the `tests/' subdirectory), they
  will be verbose and no log file will be written.
  
  You can limit the set of files using the TESTS variable, and enable
  detailed test output at the end of the test run with the VERBOSE
  variable:

    make VERBOSE=yes TESTS='first.test second.test ...' check
  
  The above command works only if you are in the `tests/' subdirectory;
  if you want to do something similar while in the top directory, you'll
  have to resort to e.g.

    env VERBOSE=yes TESTS='first.test second.test ...' make -e check

  But remember that `make -e' can be evil, since it could pull to much
  things in the makefile's namespace.  You've been warned.


Supported shells
----------------

  The test scripts are written with portability in mind, so that they
  should run an any decent bourne-compatible shell.
  
  However, some care must be used with Zsh, since, when not directly
  starting in bourne-compatibility mode, it has some incompatibilities
  in the handling of `$0' which conflict with our usage, and which have
  no easy workaround.  Thus, if you want to run a test script, say
  foo.test, with Zsh, you *can't* simply do `zsh foo.test', but you
  *must* resort to:
    zsh -o no_function_argzero foo.test

  Note that this problem does not occur if zsh is executed through a
  symlink with `sh' basename, since in that case it immediatly starts
  in bourne-compatibility mode.  So you should be perfectly safe even
  having zsh as your /bin/sh.


Preserving temporary files
--------------------------
  
  By default, every test case `foo.test' create a temporary directory
  named `foo.dir', and remove it on sucessfull termination.
  Use `keep_testdirs=yes' to keep test directories for successful tests
  also.


Reporting failures
------------------

  Send verbose output, i.e., the contents of test-suite.log, of failing
  tests to <stefano.lattarini@gmail.com>, along with the usual version
  numbers (which ratfor, which C compiler, which fortran compiler, which
  operating system, which make utility and version, which shell, etc.).
  Also report any other information which you think might be useful.



Writing test cases
==================


Do
--
  
  Make sure the new test is executable.  Add it to TESTS in Makefile.am,
  add it to XFAIL_TESTS in addition if needed.  If it depends on a data
  file (say `foo.data'), add the proper dependency in Makefile.am, e.g.
  `foo.log: foo.data', and add `foo.data' to EXTRA_DIST.
  
  Explain what each test does, at least briefly.
  
  Include `./defs.sh', aborting the test (with exit status `99') if the
  inclusion fails.  You'd better read and understand that file well before
  trying to write a test case.

  Use `set -e' to catch failures you might not have thought of.
  
  Use either `testcase_PASS', `testcase_FAIL', `testcase_SKIP' or
  `testcase_HARDERROR' to terminate a test.  If for some reason you need
  to return with a custom exit status, use `Exit' rather than `exit'.
  *Every* test must use a call to either `testcase_PASS', `testcase_FAIL',
  `testcase_SKIP', `testcase_HARDERROR' or `Exit' to do normal termination.
  If a test exits for another reason (e.g. a call to plain `exit', a failed
  command when the `set -e' switch is active, a terminating signal, a
  syntax error in shell code, ...) the test will end in an "hard error",
  and will be considered FAIL'd.
  
  Do *not* use the `testcase_*' or `Exit' subroutines in a subshell.

  Use $GREP, $FGREP, $EFREP, $SED, $AWK, $SHELL instead of the
  corresponding commands.  Also prefer $DIFF_U on plain `diff'.
  This list of tools might be incomplete: look at defs.in for a
  complete list.

  When calculating diffs on expected output and obtained output, put
  file containing expected output first, as in `$DIFF_U exp.txt got.txt'.
  This will make the output of diff more intuitive and easier to grasp.

  Use `cat' or `grep' to display (part of) files that may be
  interesting for debugging, so that when a user send a verbose
  output we don't have to ask him for more details.  Display stderr
  output on the stderr file descriptor.  If some redirected command
  is likely to fail, and `set -e' is in effect, display its output
  even in the failure case, before exiting.

  It's more important to make sure that a feature works, than
  make sure that ratfor's output looks correct.  It might look
  correct and still fail to work.  In other words, prefer compiling and
  running the fotran code generated by ratfor over grepping it (or do
  both).

  If you plan to test a ratfor feature (let's call it "foo") by both
  grepping its output and testing the generated fortran code, divide the
  test into two scripts, say `scripts/foo.test' and `scripts/foo_f77.test',
  and place any shared bata between them in `data/foo.data' (if you need
  more data files, use `data/foo.data1', `data/foo.data2' etc. instead).
  This is done so that, even when a Fortran 77 compiler is not available,
  the tests about the feature "foo" will not be completely skipped (only
  "half-skipped").  Finally, in `scripts/foo_f77.test', place a call to
  `require_fortran_compiler' just after the inclusion of `./defs.sh', so
  that the test will be skipped if no Fortran 77 compiler is available.
  
  Keep the shell code as portable as possible.  This can be trickier that
  you think; for more information, you can refer to the Autoconf manual,
  especially the section `Portable shell programming' -- which should be
  reachable at the following URL:
  http://www.gnu.org/software/autoconf/manual/html_node/Portable-Shell.html
  See also about a particularly nasty bug of zsh, in the section `Do Not'
  below.


Do not
------

  Do not make different test cases dependent one from another.  Ever.
  And above all, do not rely on a particular order in tests' execution;
  even if they are usually run in alphabetic order, this might likely no
  more hold when running them in parallel.

  Do not try to be too smart when writing a testcase: better keep it
  simple and robust.  But do not dumb down a test to the point of making
  it incomplete or sloppy.

  Do not test a ratfor error with `$RATFOR && testcase_FAIL', since the
  ratfor program might fail for a reason different from the one you thought
  of.
  Better use something like:
     run_RATFOR -e 1 || testcase_FAIL
     $GREP 'expected diagnostic' stderr || testcase_FAIL
  (Note that this doesn't prevent the test from failing for another reason,
  but at least it makes sure the original error is still there.)

  If you must run a program to later analize its stderr, do *not* run it
  by doing e.g.:
    [WRONG]  PROG 2>foo.txt
  Instead, use the run_CMD() subroutine (defined in tests/defs.sh), as in:
    [GOOD]   run_CMD PROG; mv stderr foo.txt
  You can't use the apparently simpler form `PROG 2>stderr.txt' because of
  a nasty bug of the zsh shell (at least of some 4.x versions).
  In details: if you run command when `set -x' is active, and redirect its
  standard error to a file, then zsh writes also the *trace* for the
  command into that file, instead that into the original standard error.
  For example:
    $ zsh -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    SHOULD BE EMPTY: +zsh:1> true
  while the correct behaviour is e.g.
    $ ksh -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    + true
    + 2> file
    SHOULD BE EMPTY:
  or:
    $ bash -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    + true
    SHOULD BE EMPTY:

