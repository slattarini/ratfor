HACKING file for The Ratfor Testsuite.

This file attempts to describe the rules to use when hacking/extending it.



Introduction and general infomration
====================================


 Bootstrap
 ---------

  To bootstrap checked-out sources, just run
    $ ./bootstrap.sh
  from the top directory.  This will automatically build some automake
  input files which must be dinamically generated, and then run
  `autoreconf -vi' (honoring the `AUTORECONF' environment variable).

  Note that to correctly generate the autotools' files you'll need to have
  installed the latest version of autoconf (2.64) and automake (1.11).


 Configuring and building
 ------------------------

  Run configure with the option `--enable-maintainer-make-rules', to
  enable the importing of the maintainer-specific rules (saved in files
  `maint/*.mk') in GNUmakefile.

  Run configure with the option `--enable-werror-cflag', to enable the
  `-Werror' flag in the C compiler (if supported).  This helps catching
  errors at compile-time.

  Use GNU make to enable the automatic depedencies tracking (done by
  automake) and to render the maintainer-specific make rules usable.

  Also, note that, on a cloned git repository, the configure script read in
  by default the file `maint/config.site', which contains code to make the
  `-Werror' C compiler flag and the maintainer-specific make rules enabled
  by default.  So you shouldn't need to explicitly pass to configure the
  `--enable-*' option described above.


 Recording changes
 -----------------

  * Add an adequate and clear comment for every check-in.
  * Changes other than minor bug fixes must be mentioned in NEWS.
  * Important bug fixings must be mentioned in NEWS, too.
  * If you do a change that add some obscure code for a good reason,
    please comment the code carefully and explicitly describe the resason
    that led you to write it.
  * We do not use a ChangeLog file.



Writing test cases
==================


 Do
 --

  Make sure the new test is executable.  Add it to TESTS in Makefile.am,
  add it to XFAIL_TESTS in addition if needed.  If it depends on a data
  file (say `foo.data'), add the proper dependency in Makefile.am, e.g.
  `foo.log: foo.data', and add `foo.data' to EXTRA_DIST.

  Explain what each test does, at least briefly.

  Include `./defs.sh', aborting the test (with exit status `99') if the
  inclusion fails.  You'd better read and understand that file well before
  trying to write a test case.

  Use `set -e' to catch failures you might not have thought of.

  Use the subroutine `run_CMD' if you need to run a program to later
  analyze its standard output and/or standard error.

  When running the Fortran 77 compiler, use the subroutine `run_F77'
  instead of directly calling $F77.  For example, use:
    run_F77 -- [OPTIONS] ARGS
  instead of:
    $F77 [OPTIONS] ARGS
  or:
    run_CMD $F77 [OPTIONS] [ARGS]
  Or, if you must compile only one source file (the usual case) just use:
    run_F77 foo.f
  The use of run_F77 is meant to facilitate working around possible bugs
  in supported Fortran 77 compilers.

  Do not use the Fortran builtin `stop', since some compilers has bugs
  which can lead to an incorrect behaviour of that builtin (e.g. binaries
  produced by gfortran-4.0, print "STOP 0" on stderr even if `stop' is
  called without arguments).  Use the `halt' subroutine insted (custom
  subroutine defined in auxiliary file `tests/halt.f').

  Use either `testcase_PASS', `testcase_FAIL', `testcase_SKIP' or
  `testcase_HARDERROR' to terminate a test.  If for some reason you need
  to return with a custom exit status, use `Exit' rather than `exit'.
  *Every* test must use a call to either `testcase_PASS', `testcase_FAIL',
  `testcase_SKIP', `testcase_HARDERROR' or `Exit' to do normal termination.
  If a test exits for another reason (e.g. a call to plain `exit', a failed
  command when the `set -e' switch is active, a terminating signal, a
  syntax error in shell code, ...) the test will end in an "hard error",
  and will be considered FAIL'd.

  Do *not* use the `testcase_*' or `Exit' subroutines in a subshell.

  Use $GREP, $FGREP, $EFREP, $SED, $AWK, $SHELL instead of the
  corresponding commands.  Also prefer $DIFF_U on plain `diff'.
  This list of tools might be incomplete: look at defs.in for a
  complete list.

  When calculating diffs on expected output and obtained output, put
  file containing expected output first, as in `$DIFF_U exp.txt got.txt'.
  This will make the output of diff more intuitive and easier to grasp.

  Use `cat' or `grep' to display (part of) files that may be
  interesting for debugging, so that when a user send a verbose
  output we don't have to ask him for more details.  Display stderr
  output on the stderr file descriptor.  If some redirected command
  is likely to fail, and `set -e' is in effect, display its output
  even in the failure case, before exiting.

  It's more important to make sure that a feature works, than make sure
  that ratfor's output looks correct.  It might look correct and still
  fail to work.  In other words, prefer compiling and running the fotran
  code generated by ratfor over grepping it (or do both).

  If you plan to test a ratfor feature (let's call it "foo") by both
  grepping its output and testing the generated fortran code, divide the
  test into two scripts, say `scripts/foo.test' and `scripts/foo_f77.test',
  and place any shared bata between them in `data/foo.data' (if you need
  more data files, use `data/foo.data1', `data/foo.data2' etc. instead).
  This is done so that, even when a Fortran 77 compiler is not available,
  the tests about the feature "foo" will not be completely skipped (only
  "half-skipped").  Finally, in `scripts/foo_f77.test', place a call to
  `require_fortran_compiler' just after the inclusion of `./defs.sh', so
  that the test will be skipped if no Fortran 77 compiler is available.

  Keep the shell code as portable as possible.  This can be trickier that
  you think; for more information, you can refer to the Autoconf manual,
  especially the section `Portable shell programming' -- which should be
  reachable at the following URL:
  http://www.gnu.org/software/autoconf/manual/html_node/Portable-Shell.html
  See also about a particularly nasty bug of zsh, in the section `Do Not'
  below.


 Do not
 ------

  Do not make different test cases dependent one from another.  Ever.
  And above all, do not rely on a particular order in tests' execution;
  even if they are usually run in alphabetic order, this might likely no
  more hold when running them in parallel.

  Do not try to be too smart when writing a testcase: better keep it
  simple and robust.  But do not dumb down a test to the point of making
  it incomplete or sloppy.

  Do not test a ratfor error with `$RATFOR && testcase_FAIL', since the
  ratfor program might fail for a reason different from the one you thought
  of.
  Better use something like:
     run_RATFOR -e 1 || testcase_FAIL
     $GREP 'expected diagnostic' stderr || testcase_FAIL
  (Note that this doesn't prevent the test from failing for another reason,
  but at least it makes sure the original error is still there.)

  If you must run a program to later analize its stderr, do *not* run it
  by doing e.g.:
    [WRONG]  PROG 2>foo.txt
  Instead, use the run_CMD() subroutine (defined in tests/defs.sh), as in:
    [GOOD]   run_CMD PROG; mv stderr foo.txt
  You can't use the apparently simpler form `PROG 2>stderr.txt' because of
  a nasty bug of the zsh shell (at least of some 4.x versions).
  In details: if you run command when `set -x' is active, and redirect its
  standard error to a file, then zsh writes also the *trace* for the
  command into that file, instead that into the original standard error.
  For example:
    $ zsh -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    SHOULD BE EMPTY: +zsh:1> true
  while the correct behaviour is e.g.
    $ ksh -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    + true
    + 2> file
    SHOULD BE EMPTY:
  or:
    $ bash -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    + true
    SHOULD BE EMPTY:

