HACKING file for The Ratfor Testsuite.

This file attempts to describe the rules to use when hacking/extending it.



Introduction and general infomration
====================================


 Bootstrap
 ---------

  To bootstrap checked-out sources, just run
    $ autoreconf -vi
  from the top directory. Note that to do this you'll need to have
  installed the latest version of autoconf (2.62) and automake (1.11).


 Configuring and building
 ------------------------

  Run configure with the option `--enable-maintainer-make-rules', to
  enable the importing of the maintainer-specific rules (saved in files
  `maint/*.mk') in GNUmakefile.

  Run configure with the option `--enable-werror-cflag', to enable the
  `-Werror' flag in the C compiler (if supported).  This helps catching
  errors at compile-time.

  Use GNU make to enable the automatic depedencies tracking (done by
  automake) and to render the maintainer-specific make rules usable.

  Also, note that, on a cloned git repository, the configure script read in
  by default the file `maint/config.site', which contains code to make the
  `-Werror' C compiler flag and the maintainer-specific make rules enabled
  by default.  So you shouldn't need to explicitly pass to configure the
  `--enable-*' option described above.


 Recording changes
 -----------------

  * Add an adequate and clear comment for every check-in.
  * Format the git commit message following the GNU Coding Standards for
    ChangeLog entries:
      http://www.gnu.org/prep/standards/html_node/Change-Logs.html
  * We do not use a ChangeLog file.  If needed, it can be genrated from
    `git log' output.
  * Changes other than minor bug fixes must be mentioned in NEWS.
  * Important bug fixings must be mentioned in NEWS, too.
  * If you do a change that add some obscure code for a good reason,
    please comment the code carefully and explicitly describe the resason
    that led you to write it.



Making a release
================


 Note about notation
 -------------------

  In what follows, `$VERSION' will denote the version of the package you
  are releasing.  $VERSION should be something like 0.9, 2.0, 1.11.1 or
  10.12 for stable releases, and 0.8a, 3.1b, 1.2.1c or even 3.1z for alpha
  and beta releases.  Likewise, `$RELEASE_TYPE' will denote the type of
  the release; it *must* be either `stable', `beta' or `alpha'.

 Update Git Repository
 ---------------------

  Commit any pending change to Git.  Re-bootstrap and re-configure the
  package.  Be sure you have enabled the import of maintainer make rules.
  Run the extended testsuite of the package, with `make strict-distcheck'.
  If this fail, fix the errors, commit to Git and repeat the procedure,
  until `make strict-distcheck' succeed.

  Cite the new $VERSION in the `NEWS' file, and commit that change (with
  a message like "NEWS: version bumped to $VERSION").  Create an annotated
  git tag named *exactly* `v$VERSION' (e.g. `v1.0' or `v2.1e'), and with a
  comment that is *exactly* `$RELEASE_TYPE release $VERSION' (e.g. `stable
  release 1.0' or `beta release 2.1e').

 Build the tarball of the release
 --------------------------------

  Then simply issue a `make alpha' (if releasing an alpha version), `make
  beta' (if releasing a beta version), or `make stable' (if releasing a
  stable version).  This will do some sanity checks on the state of your
  repository (mostly to verify that you took the steps listed above), and
  if this passes will create a distribution tarball with `make distcheck'.



Writing test cases
==================


 Do
 --

  Make sure the new test is executable.  Add it to TESTS in Makefile.am,
  add it to XFAIL_TESTS in addition if needed.  If it depends on a data
  file (say `foo.data'), add the proper dependency in Makefile.am, e.g.
  `foo.log: foo.data', and add `foo.data' to EXTRA_DIST.

  Explain what each test does, at least briefly.

  Include `./TestsAux/defs.sh', aborting the test (with exit status `99')
  if the inclusion fails.  You'd better read and understand that file well
  before trying to write a test case.

  Keep in mind that `defs.sh' calls `set -e', so that failures you might
  not have thought of are still catched (and result in an "hard error").
  If this is not what you want, you'll have to explicitly call `set +e'
  in your test script (this is generally a bad idea, unless you need to
  *temporarly* disable the exit-on-error flag for some good reason).

  Use the subroutine `run_command' if you need to run a program to later
  analyze its standard output and/or standard error.

  When running the ratfor preprocessor, use the subroutine `run_RATFOR'
  instead of directly calling $RATOR.  For example, use:
    run_RATFOR -- [OPTIONS] ARGS
  instead of:
    $RATFOR [OPTIONS] ARGS
  or even of:
    run_command $RATFOR [OPTIONS] [ARGS]
  See the documentation of run_RATFOR for more information.

  When running the Fortran 77 compiler, use the subroutine `run_F77'
  instead of directly calling $F77.  For example, use:
    run_F77 -- [OPTIONS] ARGS
  instead of:
    $F77 [OPTIONS] ARGS
  or even of:
    run_command $F77 [OPTIONS] [ARGS]
  Or, if you must compile only one source file (the usual case) just use:
    run_F77 foo.f
  The use of run_F77 is meant to facilitate working around possible bugs
  in supported Fortran 77 compilers.  See the documentation of run_F77 for
  more information.

  Do not use the Fortran builtin `stop', since some compilers has bugs
  which can lead to an incorrect behaviour of that builtin (e.g. binaries
  produced by gfortran-4.0, print "STOP 0" on stderr even if `stop' is
  called without arguments).  Use the `halt' subroutine insted (custom
  subroutine defined in auxiliary file `tests/halt.f').

  Use `testcase_SKIP' to make a test script to be SKIP'd (also, provide a
  reason for this skip, by passing a proper argument to testcase_SKIP).
  Use `testcase_REGISTER_FAILURE' to register a non-fatal failure (which
  shouldn't stop the test script immediately).
  Use `testcase_FATAL_FAILURE' to issue a fatal failure (which should
  cause the test script to exit immediately).
  Use `testcase_HARDERROR' to issue a fatal error (e.g. when some
  unexpected condition is met, or when some auxiliary command fails
  unexpectedly -- obviously, this causes the immediate termination of
  the test script).
  Use `testcase_DONE' to cause normal termination of the test script (this
  will cause the test to PASS if no failure has been encounterd, and will
  cause it to FAIL otherwise).

  Do *not* use any of the the `testcase_*' subroutines described above in
  a subshell.

  Note that *every* test must use a call to either `testcase_DONE',
  `testcase_SKIP' or `testcase_FATAL_FAILURE' to do normal termination.
  If a test exits for another reason (e.g. a call to `testcase_HARDERROR',
  a call to plain `exit', a failed command when the `set -e' switch is
  active, a terminating signal, a syntax error in shell code, ...) the
  test will end in an "hard error", and will be considered FAIL'd.

  Use $GREP, $FGREP, $EGREP, $SED, $AWK, instead of the corresponding
  commands.  Also prefer $DIFF_U on plain `diff'.  This list of tools might
  be incomplete: look in the file `tests/TestsAux/defs.in' for a complete
  list.

  When calculating diffs on expected output and obtained output, put
  file containing expected output first, as in `$DIFF_U exp.txt got.txt'.
  This will make the output of diff more intuitive and easier to grasp.

  Use `cat' or `grep' to display (part of) files that may be
  interesting for debugging, so that when a user send a verbose
  output we don't have to ask him for more details.  Display stderr
  output on the stderr file descriptor.  If some redirected command
  is likely to fail, and `set -e' is in effect, display its output
  even in the failure case, before exiting.

  It's more important to make sure that a feature works, than make sure
  that ratfor's output looks correct.  It might look correct and still
  fail to work.  In other words, prefer compiling and running the Fortran
  code generated by ratfor over grepping it (or do both).

  If you plan to test a ratfor feature (let's call it "foo") by both
  grepping its output and testing the generated Fortran code, divide the
  test into two scripts, say `scripts/foo.test' and `scripts/foo_f77.test',
  and place any shared bata between them in `data/foo.data' (if you need
  more data files, use `data/foo.data1', `data/foo.data2' etc. instead).
  This is done so that, even when a Fortran 77 compiler is not available,
  the tests about the feature "foo" will not be completely skipped (only
  "half-skipped").  Finally, in `scripts/foo_f77.test', place a call to
  `require_fortran_compiler' just after the inclusion of `defs.sh', so
  that the test will be skipped if no Fortran 77 compiler is available.

  Keep the shell code as portable as possible.  This can be trickier that
  you think; for more information, you can refer to the Autoconf manual,
  especially the section `Portable shell programming' -- which should be
  reachable at the following URL:
  http://www.gnu.org/software/autoconf/manual/html_node/Portable-Shell.html
  See also about a particularly nasty bug of zsh, in the section `Do Not'
  below.


 Do not
 ------

  Do not make different test cases dependent one from another.  Ever.
  And above all, do not rely on a particular order in tests' execution;
  even if they are usually run in alphabetic order, this might likely no
  more hold when running them in parallel.

  Do not try to be too smart when writing a testcase: better keep it
  simple and robust.  But do not dumb down a test to the point of making
  it incomplete or sloppy.

  Do not test a ratfor error with `$RATFOR && testcase_FAIL', since the
  ratfor program might fail for a reason different from the one you thought
  of.
  Better use something like:
     run_RATFOR -e 1 || testcase_FAIL
     $GREP 'expected diagnostic' stderr || testcase_FAIL
  (Note that this doesn't prevent the test from failing for another reason,
  but at least it makes sure the original error is still there.)

  If you must run a program to later analize its stderr, do *not* run it
  by doing e.g.:
    [WRONG]  PROG 2>foo.txt
  Instead, use the `run_command' subroutine (defined in the file
  `tests/TestsAux/defs.in'), as in:
    [GOOD]   run_command PROG; mv stderr foo.txt
  You can't use the apparently simpler form `PROG 2>stderr.txt' because
  of a nasty bug of the zsh shell (at least of some 4.x versions).
  In details: if you run command when `set -x' is active, and redirect
  its standard error to a file, then zsh writes also the *trace* for the
  command into that file, instead that into the original standard error.
  For example:
    $ zsh -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    SHOULD BE EMPTY: +zsh:1> true
  while the correct behaviour is e.g.
    $ ksh -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    + true
    + 2> file
    SHOULD BE EMPTY:
  or:
    $ bash -c -x 'true 2>file'; echo "SHOULD BE EMPTY: `cat file`"
    + true
    SHOULD BE EMPTY:

